{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73806a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a870653",
   "metadata": {},
   "source": [
    "# Merge Gen and NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2b32a",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nlp_genai_data(input_folder_nlp, input_folder_genai, output_folder):\n",
    "    file_list = sorted(os.listdir(input_folder_nlp), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # Iterate every file by number\n",
    "    for file_name in tqdm(file_list, desc=\"衝鴨!!!\"):\n",
    "        file_use_nlp = os.path.join(input_folder_nlp, file_name)\n",
    "        file_use_genai = os.path.join(input_folder_genai, file_name)\n",
    "\n",
    "        df_nlp = pd.read_excel(file_use_nlp)\n",
    "        df_nlp.drop(columns=[\"sentence\"], inplace=True)\n",
    "\n",
    "        df_genai = pd.read_excel(file_use_genai)\n",
    "        df_genai.drop(columns=[\"number\", \"sentence\"], inplace=True)\n",
    "\n",
    "        df = pd.concat([df_nlp, df_genai], axis=1)        \n",
    "        df.to_excel(os.path.join(output_folder, file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28359c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment \n",
    "folder_nlp = r\"Sentiment_result_NLP\"\n",
    "folder_genai = r\"Sentiment_result_genAI\"\n",
    "sentiment_output_folder = r\"Sentiment_ALL\"\n",
    "\n",
    "merge_nlp_genai_data(folder_nlp, folder_genai, sentiment_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b08985",
   "metadata": {},
   "source": [
    "## ESG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d72e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nlp_genai_data_esg(input_folder_nlp, input_folder_genai, output_folder):\n",
    "    file_list = sorted(os.listdir(input_folder_nlp), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # Iterate every file by number\n",
    "    for file_name in tqdm(file_list, desc=\"衝鴨!!!\"):\n",
    "        file_use_nlp = os.path.join(input_folder_nlp, file_name)\n",
    "        file_use_genai = os.path.join(input_folder_genai, file_name)\n",
    "\n",
    "        df_nlp = pd.read_excel(file_use_nlp)\n",
    "        df_nlp.drop(columns=[\"sentence\"], inplace=True)\n",
    "\n",
    "        df_genai = pd.read_excel(file_use_genai)\n",
    "        df_genai.drop(columns=[\"number\", \"sentence\"], inplace=True)\n",
    "\n",
    "        df = pd.concat([df_nlp, df_genai], axis=1) \n",
    "        df.to_excel(os.path.join(output_folder, file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESG \n",
    "folder_nlp = r\"ESG_result_NLP\"\n",
    "folder_genai = r\"ESG_result_genAI\"\n",
    "output_folder = r\"ESG_ALL\" \n",
    "\n",
    "merge_nlp_genai_data_esg(folder_nlp, folder_genai, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882c057",
   "metadata": {},
   "source": [
    "# For Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157b50e",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72323c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_to_one(variable, input_folder):\n",
    "    file_list = sorted(os.listdir(input_folder), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # Iterate every file by number\n",
    "    df_list = []\n",
    "    for file_name in tqdm(file_list, desc=\"衝鴨!!!\"):\n",
    "        file_use_nlp = os.path.join(input_folder, file_name)\n",
    "\n",
    "        df = pd.read_excel(file_use_nlp)\n",
    "        df.drop(columns=[\"number\"], inplace=True)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df_agg = pd.concat(df_list, axis=0)\n",
    "    df_agg.to_excel(f\"{variable}_Metric.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "sentiment_folder = r\"Sentiment_ALL\"\n",
    "agg_to_one(\"Sentiment\" ,sentiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_folder = r\"ESG_ALL\"\n",
    "agg_to_one(\"ESG\", esg_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14167fb4",
   "metadata": {},
   "source": [
    "# For Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a2e5a",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf00713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calcuate__sentiment_variable(input_folder):\n",
    "    file_list = sorted(os.listdir(input_folder), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # Make dic object avaliable for append() func\n",
    "    result_net_tone = defaultdict(list)\n",
    "    result_power = defaultdict(list)\n",
    "\n",
    "    # Iterate every file by number\n",
    "    for file_name in tqdm(file_list, desc=\"衝鴨!!!\"):\n",
    "        file_use_nlp = os.path.join(input_folder, file_name)\n",
    "\n",
    "        df = pd.read_excel(file_use_nlp)\n",
    "        df.drop(columns=[\"number\", \"researcher_sentiment\"], inplace=True)\n",
    "        total_count = len(df)\n",
    "        for model in df.columns:\n",
    "            positive_count = (df[model] == 1).sum()\n",
    "            negative_count = (df[model] == 2).sum()\n",
    "\n",
    "            net_tone = (positive_count - negative_count) / total_count\n",
    "            sentiment_power = (positive_count + negative_count) / total_count\n",
    "\n",
    "            result_net_tone[model].append(net_tone)\n",
    "            result_power[model].append(sentiment_power)\n",
    "        \n",
    "    result_net_tone = dict(result_net_tone)\n",
    "    result_power = dict(result_power)\n",
    "\n",
    "    df_net_tone = pd.DataFrame(result_net_tone)\n",
    "    df_power = pd.DataFrame(result_power)\n",
    "\n",
    "    df_net_tone.to_excel(\"Net_Tone.xlsx\", index=False)\n",
    "    df_power.to_excel(\"Sentiment_Power.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_folder = r\"Sentiment_ALL\"\n",
    "\n",
    "calcuate__sentiment_variable(sentiment_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee2b0c",
   "metadata": {},
   "source": [
    "# Metric 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metrics_1 = evaluate.load(\"accuracy\")\n",
    "\n",
    "metrics_2 = evaluate.combine([\n",
    "    evaluate.load(\"precision\"),          \n",
    "    evaluate.load(\"recall\"),\n",
    "    evaluate.load(\"f1\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1dda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.read_excel(r\"Sentiment_Metric.xlsx\")\n",
    "df_esg = pd.read_excel(r\"ESG_Metric.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057f6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calculate(preds, labels): \n",
    "    results = metrics_2.compute(\n",
    "        predictions=preds,\n",
    "        references=labels,\n",
    "        average=\"macro\"         \n",
    "    ) | metrics_1.compute(\n",
    "        predictions=preds,\n",
    "        references=labels\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a53de4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_run(df_variable):\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    labels = df_variable[df_variable.columns[0]]\n",
    "    for model in tqdm(df_variable.columns[1:], desc=\"我會走\"):\n",
    "        preds = df_variable[model]\n",
    "        result = metric_calculate(preds, labels)\n",
    "        accuracy_list.append(result[\"accuracy\"])\n",
    "        precision_list.append(result[\"precision\"])\n",
    "        recall_list.append(result[\"recall\"])\n",
    "        f1_list.append(result[\"f1\"])\n",
    "    \n",
    "    df = pd.DataFrame([accuracy_list, precision_list, recall_list, f1_list], \n",
    "                        columns=df_variable.columns[1:], \n",
    "                        index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_run(df_esg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
